{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**NExt-GPT: Any-to-Any Multimodal LLM**\n",
        "\n",
        "*   [NexT-GPT](https://github.com/NExT-GPT/NExT-GPT)\n",
        "*   [Folder](https://drive.google.com/drive/folders/12P7iQLTxXuMhlY8ArzVogIRWujy0_ldr?usp=sharing)"
      ],
      "metadata": {
        "id": "WLlK9PuIdgeW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GW8w1Kp1F_MY",
        "outputId": "ba120191-947d-4657-dc5c-87bc88f349c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/NExT-GPT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkJCUkfSGZXQ",
        "outputId": "021eaef4-9439-4ab2-a765-2b6cd66104a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/12P7iQLTxXuMhlY8ArzVogIRWujy0_ldr/NExT-GPT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -r requirements.txt"
      ],
      "metadata": {
        "id": "-z_8kYtBGrPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"jax[cuda12_local]==0.4.23\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70RSyGeZUC6V",
        "outputId": "c00de833-b868-4a20-c864-d9e722f6b3e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
            "Requirement already satisfied: jax[cuda12_local]==0.4.23 in /usr/local/lib/python3.10/dist-packages (0.4.23)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax[cuda12_local]==0.4.23) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from jax[cuda12_local]==0.4.23) (1.24.3)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax[cuda12_local]==0.4.23) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax[cuda12_local]==0.4.23) (1.11.2)\n",
            "Requirement already satisfied: jaxlib==0.4.23+cuda12.cudnn89 in /usr/local/lib/python3.10/dist-packages (from jax[cuda12_local]==0.4.23) (0.4.23+cuda12.cudnn89)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/NExT-GPT/code"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDDJDp0NTWo_",
        "outputId": "3d4f09ec-b025-418e-fabf-38e15d3c3671"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/12P7iQLTxXuMhlY8ArzVogIRWujy0_ldr/NExT-GPT/code\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Config**"
      ],
      "metadata": {
        "id": "P912Zfn0d1kq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from config import *"
      ],
      "metadata": {
        "id": "pz_FpsVURWMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g_cuda = torch.Generator(device='cuda').manual_seed(1337)\n",
        "\n",
        "args = {'model': 'nextgpt',\n",
        "        'nextgpt_ckpt_path': '/content/drive/.shortcut-targets-by-id/12P7iQLTxXuMhlY8ArzVogIRWujy0_ldr/NExT-GPT/ckpt/delta_ckpt/nextgpt/7b_tiva_v0',\n",
        "        'max_length': 128,\n",
        "        'stage': 3,\n",
        "        'root_dir': '/content/drive/.shortcut-targets-by-id/12P7iQLTxXuMhlY8ArzVogIRWujy0_ldr/NExT-GPT',\n",
        "        'mode': 'validate',\n",
        "        'pretrained_ckpt_path': 'content/drive/.shortcut-targets-by-id/12P7iQLTxXuMhlY8ArzVogIRWujy0_ldr/NExT-GPT/ckpt/pretrained_ckpt/',\n",
        "        }\n",
        "args.update(load_config(args))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVFYXQzbPBII",
        "outputId": "ef43976f-d6f1-4d29-d208-d0b1888fa6ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[!] load base configuration: config/base.yaml\n",
            "[!] load configuration from config/stage_3.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args['pretrained_ckpt_path']= '/content/drive/.shortcut-targets-by-id/12P7iQLTxXuMhlY8ArzVogIRWujy0_ldr/NExT-GPT/ckpt/pretrained_ckpt/'"
      ],
      "metadata": {
        "id": "I2kBDM51Sj19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7vzv8tNONID",
        "outputId": "dec98932-6ee5-427c-a1d9-b9f7c841c3f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model': 'nextgpt',\n",
              " 'nextgpt_ckpt_path': '/content/drive/.shortcut-targets-by-id/12P7iQLTxXuMhlY8ArzVogIRWujy0_ldr/NExT-GPT/ckpt/delta_ckpt/nextgpt/7b_tiva_v0',\n",
              " 'max_length': 512,\n",
              " 'stage': 3,\n",
              " 'root_dir': '/content/drive/.shortcut-targets-by-id/12P7iQLTxXuMhlY8ArzVogIRWujy0_ldr/NExT-GPT',\n",
              " 'mode': 'validate',\n",
              " 'pretrained_ckpt_path': '/content/drive/.shortcut-targets-by-id/12P7iQLTxXuMhlY8ArzVogIRWujy0_ldr/NExT-GPT/ckpt/pretrained_ckpt/',\n",
              " 'models': {'nextgpt': {'model_name': 'NextGPTModel',\n",
              "   'agent_name': 'DeepSpeedAgent'}},\n",
              " 'seed': 13,\n",
              " 'logging_step': 5,\n",
              " 'num_clip_tokens': 77,\n",
              " 'gen_emb_dim': 768,\n",
              " 'vicuna_version': '7b_v0',\n",
              " 'imagebind_version': 'huge',\n",
              " 'n_img_tokens': 4,\n",
              " 'text_emb_to_img_layers': [-1],\n",
              " 'num_gen_img_tokens': 4,\n",
              " 'text_fc_to_img_mode': 'transformer',\n",
              " 'n_video_tokens': 24,\n",
              " 'text_emb_to_video_layers': [-1],\n",
              " 'num_gen_video_tokens': 24,\n",
              " 'text_fc_to_video_mode': 'transformer',\n",
              " 'n_audio_tokens': 8,\n",
              " 'text_emb_to_audio_layers': [-1],\n",
              " 'num_gen_audio_tokens': 8,\n",
              " 'text_fc_to_audio_mode': 'transformer',\n",
              " 'image_diffusion': 'runwayml/stable-diffusion-v1-5',\n",
              " 'video_diffusion': 'cerspense/zeroscope_v2_576w',\n",
              " 'audio_diffusion': 'cvssp/audioldm-l-full',\n",
              " 'lora_r': 32,\n",
              " 'lora_alpha': 32,\n",
              " 'lora_dropout': 0.1,\n",
              " 'freeze_lm': False,\n",
              " 'freeze_input_proj': False,\n",
              " 'freeze_output_proj': False,\n",
              " 'prompt': ''}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Model**"
      ],
      "metadata": {
        "id": "B3hmo_C6d9Kn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from model.anyToImageVideoAudio import NextGPTModel\n",
        "\n",
        "model = NextGPTModel(**args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GI4YV9oTIWC",
        "outputId": "745b8aa9-0788-433e-81cf-9d63f0e0bfe6"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting ds_accelerator to cuda (auto detect)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
            "  warnings.warn(\n",
            "/content/drive/.shortcut-targets-by-id/12P7iQLTxXuMhlY8ArzVogIRWujy0_ldr/NExT-GPT/code/model/custom_sd.py:28: FutureWarning: Importing `DiffusionPipeline` or `ImagePipelineOutput` from diffusers.pipeline_utils is deprecated. Please import from diffusers.pipelines.pipeline_utils instead.\n",
            "  from diffusers.pipeline_utils import DiffusionPipeline\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "args max_length 512\n",
            "Initializing visual encoder from /content/drive/.shortcut-targets-by-id/12P7iQLTxXuMhlY8ArzVogIRWujy0_ldr/NExT-GPT/ckpt/pretrained_ckpt/imagebind_ckpt/huge ...\n",
            "Visual encoder initialized.\n",
            "Initializing language decoder from /content/drive/.shortcut-targets-by-id/12P7iQLTxXuMhlY8ArzVogIRWujy0_ldr/NExT-GPT/ckpt/pretrained_ckpt/vicuna_ckpt/7b_v0 ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "delta_ckpt = torch.load(os.path.join(args['nextgpt_ckpt_path'], 'pytorch_model.pt'), map_location=torch.device('cuda'))\n",
        "model.load_state_dict(delta_ckpt, strict=False)\n",
        "model = model.eval().half().cuda()"
      ],
      "metadata": {
        "id": "QnYjWPvlaHcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(\n",
        "    input,\n",
        "    image_path=None,\n",
        "    audio_path=None,\n",
        "    video_path=None,\n",
        "    thermal_path=None,\n",
        "    max_tgt_len=200,\n",
        "    top_p=10.0,\n",
        "    temperature=0.1,\n",
        "    history=None,\n",
        "    modality_cache=None,\n",
        "    filter_value=-float('Inf'), min_word_tokens=0,\n",
        "    gen_scale_factor=10.0, max_num_imgs=1,\n",
        "    stops_id=None,\n",
        "    load_sd=True,\n",
        "    generator=None,\n",
        "    guidance_scale_for_img=7.5,\n",
        "    num_inference_steps_for_img=50,\n",
        "    guidance_scale_for_vid=7.5,\n",
        "    num_inference_steps_for_vid=50,\n",
        "    max_num_vids=1,\n",
        "    height=320,\n",
        "    width=576,\n",
        "    num_frames=24,\n",
        "    guidance_scale_for_aud=7.5,\n",
        "    num_inference_steps_for_aud=50,\n",
        "    max_num_auds=1,\n",
        "    audio_length_in_s=9,\n",
        "    ENCOUNTERS=1,\n",
        "):\n",
        "    if image_path is None and audio_path is None and video_path is None and thermal_path is None:\n",
        "        # return [(input, \"There is no input data provided! Please upload your data and start the conversation.\")]\n",
        "        print('no image, audio, video, and thermal are input')\n",
        "    else:\n",
        "        print(\n",
        "            f'[!] image path: {image_path}\\n[!] audio path: {audio_path}\\n[!] video path: {video_path}\\n[!] thermal path: {thermal_path}')\n",
        "\n",
        "    # prepare the prompt\n",
        "    prompt_text = ''\n",
        "    if history != None:\n",
        "        for idx, (q, a) in enumerate(history):\n",
        "            if idx == 0:\n",
        "                prompt_text += f'{q}\\n### Assistant: {a}\\n###'\n",
        "            else:\n",
        "                prompt_text += f' Human: {q}\\n### Assistant: {a}\\n###'\n",
        "        prompt_text += f'### Human: {input}'\n",
        "    else:\n",
        "        prompt_text += f'### Human: {input}'\n",
        "\n",
        "    print('prompt_text: ', prompt_text)\n",
        "\n",
        "    response = model.generate({\n",
        "        'prompt': prompt_text,\n",
        "        'image_paths': [image_path] if image_path else [],\n",
        "        'audio_paths': [audio_path] if audio_path else [],\n",
        "        'video_paths': [video_path] if video_path else [],\n",
        "        'thermal_paths': [thermal_path] if thermal_path else [],\n",
        "        'top_p': top_p,\n",
        "        'temperature': temperature,\n",
        "        'max_tgt_len': max_tgt_len,\n",
        "        'modality_embeds': modality_cache,\n",
        "        'filter_value': filter_value, 'min_word_tokens': min_word_tokens,\n",
        "        'gen_scale_factor': gen_scale_factor, 'max_num_imgs': max_num_imgs,\n",
        "        'stops_id': stops_id,\n",
        "        'load_sd': load_sd,\n",
        "        'generator': generator,\n",
        "        'guidance_scale_for_img': guidance_scale_for_img,\n",
        "        'num_inference_steps_for_img': num_inference_steps_for_img,\n",
        "\n",
        "        'guidance_scale_for_vid': guidance_scale_for_vid,\n",
        "        'num_inference_steps_for_vid': num_inference_steps_for_vid,\n",
        "        'max_num_vids': max_num_vids,\n",
        "        'height': height,\n",
        "        'width': width,\n",
        "        'num_frames': num_frames,\n",
        "\n",
        "        'guidance_scale_for_aud': guidance_scale_for_aud,\n",
        "        'num_inference_steps_for_aud': num_inference_steps_for_aud,\n",
        "        'max_num_auds': max_num_auds,\n",
        "        'audio_length_in_s': audio_length_in_s,\n",
        "        'ENCOUNTERS': ENCOUNTERS,\n",
        "\n",
        "    })\n",
        "    return response"
      ],
      "metadata": {
        "id": "uU9vwz5Fae9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_tgt_length = 150\n",
        "top_p = 1.0\n",
        "temperature = 0.4\n",
        "modality_cache = None\n",
        "\n",
        "prompt = 'Generate a image : woman walk a dop in the park.'\n",
        "\n",
        "history = []\n",
        "\n",
        "output = predict(\n",
        "    input=prompt,\n",
        "    history=history,\n",
        "    max_tgt_len=max_tgt_length, top_p=top_p,\n",
        "    temperature=temperature, modality_cache=modality_cache,\n",
        "    filter_value=-float('Inf'), min_word_tokens=10,\n",
        "    gen_scale_factor=4.0, max_num_imgs=1,\n",
        "    stops_id=[[835]],\n",
        "    load_sd=True,\n",
        "    generator=g_cuda,\n",
        "    guidance_scale_for_img=7.5,\n",
        "    num_inference_steps_for_img=50,\n",
        "    guidance_scale_for_vid=7.5,\n",
        "    num_inference_steps_for_vid=50,\n",
        "    max_num_vids=1,\n",
        "    height=320,\n",
        "    width=576,\n",
        "    num_frames=24,\n",
        "    ENCOUNTERS=1\n",
        "    )"
      ],
      "metadata": {
        "id": "uV8waQlZaZth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Parser**"
      ],
      "metadata": {
        "id": "M0gAc4zleIDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers.utils import export_to_video\n",
        "import scipy\n",
        "\n",
        "for i in output:\n",
        "    if isinstance(i, str):\n",
        "        print(i)\n",
        "    elif 'img' in i.keys():\n",
        "        for m in i['img']:\n",
        "            if isinstance(m, str):\n",
        "                print(m)\n",
        "            else:\n",
        "                m[0].save(f'./results/{prompt}.jpg')\n",
        "\n",
        "    elif 'vid' in i.keys():\n",
        "        for idx, m in enumerate(i['vid']):\n",
        "            if isinstance(m, str):\n",
        "                print(m)\n",
        "            else:\n",
        "                video_path = export_to_video(video_frames=m, output_video_path=f'./results/{prompt}.mp4')\n",
        "                print(\"video_path: \", video_path)\n",
        "    elif 'aud' in i.keys():\n",
        "        for idx, m in enumerate(i['aud']):\n",
        "            if isinstance(m, str):\n",
        "                print(m)\n",
        "            else:\n",
        "                audio_path = f'./results/{prompt}.wav'\n",
        "                scipy.io.wavfile.write(audio_path, rate=16000, data=m)\n",
        "                print(\"video_path: \", audio_path)\n",
        "    else:\n",
        "        pass"
      ],
      "metadata": {
        "id": "43OgwG1dayrf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}